---
layout: article
title: 01. CS231N ìš”ì•½ (Background, Neural network)
author: J_å®‹
tags: ComputerVision CS231n í•œê¸€
mathjax: true
key: ComputerVision01
---



#### Image 

- ì´ë¯¸ì§€ëŠ” ìˆ«ìë¡œ êµ¬ì„±ëœ 3D Arrayë¡œ êµ¬ì„±ë¨
  - ìˆ«ì = 0~255
  - 3D Array = [Hight] Ã— [Width] Ã— [Color channel]



#### Challenges 

1. ë³´ëŠ” ì‹œê°ì— ë”°ë¼ ì´ë¯¸ì§€ëŠ” ë‹¤ë¥´ê²Œ ë³´ì¼ ìˆ˜ ìˆìŒ
2. ì¡°ëª…ì˜ ë°ê¸°/ìƒ‰ìƒì— ë”°ë¼ ë‹¤ë¥´ê²Œ ë³´ì¼ ìˆ˜ ìˆìŒ
3. ì¸ì‹í•˜ë ¤ê³ í•˜ëŠ” ëŒ€ìƒì´ í•­ìƒ ì •ë©´ì„ ì„œìˆëŠ”ê²Œ ì•„ë‹˜. ë¹„í‹€ì–´ì ¸ìˆê³  íšŒì „í•´ìˆê³  ëˆ„ì›Œìˆê³ 
4. ì¸ì‹ëŒ€ìƒì´ ì€íì—„í/ê°€ë ¤ì§ ë“±ìœ¼ë¡œ ì¼ë¶€ë§Œ ì¸ì‹ ê°€ëŠ¥
5. ë°°ê²½í˜¼ë€, ì¸ì‹ëŒ€ìƒë¬¼ê³¼ ë°°ê²½ì˜ íŒ¨í„´ì´ ë¹„ìŠ·
6. ë™ì¢… ë‚´ ë‹¤ì–‘ì„±, ê°™ì€ ê³ ì–‘ì´ë¼ë„ í˜ë¥´ì‹œì•„ê³ ì–‘ì´, ê¸¸ê³ ì–‘ì´ ë‹¤ ë‹¤ë¥´ê²Œìƒê¹€



####  Challenges ì ‘ê·¼ ë°©ë²• 

1. ì´ë¯¸ì§€ì˜ íŠ¹ì§• íŒŒì•…
   - íŠ¹ì§•ì ì¸ ì—£ì§€ë‚˜ ëª¨í˜•ì„ ì°¾ì•„ì„œ Libraryí™” ì‹œí‚´ ê·¸ë¦¬ê³  ë°°ì—´ ìƒíƒœ ë¹„êµ í›„ Classification í•¨
   - ì„±ëŠ¥ì´ ë‚®ìŒ

2. ë°ì´í„° ê¸°ë°˜ ì ‘ê·¼

   - ì´ë¯¸ì§€ì™€ ë ˆì´ë¸”ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì…‹ì„ ìˆ˜ì§‘

   - ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ ì´ë¯¸ì§€ Classifierë¥¼ í•™ìŠµì‹œí‚´

     ```python
     def train(train_images, train_labels):
       ...
       return model
     ```

   - Test ì´ë¯¸ì§€ì…‹ë“¤ì— ëŒ€í•´ì„œ í•™ìŠµì‹œí‚¨ ì´ë¯¸ì§€ Classifierë¥¼ í‰ê°€ 

     ```python
     def predict(model, test_images):
       ...
       return test_labels
     ```



#### Nearest Neighbor Classifier

- Training ë‹¨ê³„ : 
  - ëª¨ë“  í•™ìŠµ ë°ì´í„°ë“¤ì„ ë©”ëª¨ë¦¬ ìƒì— ì˜¬ë ¤ì„œ ê¸°ì–µí•˜ê²Œ ë¨
- ì˜ˆì¸¡ ë‹¨ê³„ :
  - Test ì´ë¯¸ì§€ í•œì¥ì„  L1 Distanceë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  í•™ìŠµ ë°ì´í„°ë“¤ê³¼ ë¹„êµ
  - L1 Distance ê°’ì´ ê°€ì¥ ì‘ì€ í•™ìŠµ ë°ì´í„°ë¥¼ ì°¾ì•„ Test ì´ë¯¸ì§€ì˜ ë ˆì´ë¸”ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŒ
- ë‹¨ì  :
  - í•™ìŠµë°ì´í„° ì–‘ì— ë”°ë¼ Classification ì‘ì—… (ì˜ˆì¸¡ ë‹¨ê³„) ì‹œê°„ì´ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•˜ê²Œë¨
  -  ì†ë„ë¥¼ ë¹¨ë¦¬í•  ìˆ˜ ìˆëŠ” Nearest Neighbor Classifierë“¤ : ANN, FLANN



K-Nearest Neighbor Classifier

- Kê°œì˜ ê·¼ì ‘í•œ í•™ìŠµë°ì´í„°ë¥¼ ì°¾ëŠ” ë°©ì‹

- ë³´í†µ 1-Nearest Neighborë³´ë‹¤ ì„±ëŠ¥ì´ ì¢€ ë” ì¢‹ìŒ
- ê·¼ë° í˜„ì‹¤ì—ì„œëŠ” ì‚¬ìš©í•´ì„œëŠ” ì•ˆë¨ 



#### Linear Classification (Score function)

- ì´ë¯¸ì§€ ë‚´ì˜ ëª¨ë“  Pixelê°’ ë“¤ì— ëŒ€í•´ì„œ Weightë¥¼ ê³±í•˜ì—¬ì„œ ì²˜ë¦¬ í•œ ê²ƒì˜ "í•©"ì´ë‹¤ 

  <img src="/assets/images/è®¡è§†/myNote/pic01/Screen Shot 2021-07-24 at 9.53.31 PM.png" alt="Screen Shot 2021-07-24 at 9.53.31 PM" style="zoom: 100%;" />

  - Linear Classificationë¥¼ í†µí•´ ì…ë ¥ ì´ë¯¸ì§€ì˜ ê° classì— ëŒ€í•œ scoreë¥¼ ë‚¼ ìˆ˜ ìˆìŒ

  

#### Loss Function 

- Linear Classificationë¥¼ í†µí•´ ë‚˜ì˜¨ scoreë“¤ì´ ì–´ëŠ ì •ë„ ì¢‹ê±°ë‚˜ ë˜ëŠ” ë‚˜ì˜ëƒì˜ ì •ë„ë¥¼ ì •ëŸ‰í™” í•˜ëŠ” ê²ƒ

- ***Example 1)*** SVM Hinge loss

  ```python
  """
  SVM ì˜ Hinge Lossë¥¼ ì‚¬ìš©í•˜ì—¬ Loss ê°’ êµ¬í•˜ê¸°
  x : Input image
  y : Input image label
  W : Score function's Weight
  """
  def Li_vectorized(x, y, W):
    scores = W.dot(x)
    margins = np.maximum(0, scores-scores[y]+1) 
    margins[y] = 0
    loss_i = np.sum(margins) # np.meanì„ ì¨ë„ í° ì˜ë¯¸ëŠ” ì—†ìŒ, ë‹¨ì§€ scaleë§Œ ì‘ì•„ì§ 
    return loss_i
  ```

   <img src="/assets/images/è®¡è§†/myNote/pic01/Screen Shot 2021-07-24 at 11.20.02 PM.png" alt="Screen Shot 2021-07-24 at 11.20.02 PM" style="zoom:30%;" />

  - `Li_vectorized(x, y, W)` returnê°’ ìµœì†Œ/ìµœëŒ€  =  0 / âˆ ï¼ˆåˆ™, Losså€¼ä¸º'0' è¡¨ç¤ºæœ€å¥½ï¼‰

  - íŠ¹ì„± : ë¯¸ì„¸í•œ ë°ì´í„° ë³€ê²½ì— ë‘”í•œ í¸, ë‹¨ì§€ ì •ë‹µ classê°€ ë‹¤ë¥¸ classë³´ë‹¤ ë†’ëƒì— ì´ˆì ì„ ë‘ 

    

- ***Example 2)*** Softmax Cross entropy loss

  - Softmax function : Score functionì„ í†µí•´ ë‚˜ì˜¨ score vectorì˜ ê° elementê°’ë“¤ 0~1ì‚¬ì´ë¡œ ë³€ê²½

    - ë³€ê²½ëœ elementê°’ë“¤ì˜ ì´ í•©ì€ 1

    ```python
    import numpy as np
    # Score functionì„ í†µí•´ ë‚˜ì˜¨ score vector
    f = np.array([3.2, 5.1, -1.7]) 
    # Numeric problemë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ 
    f = f - np.max(f)
    # Softmax function
    p = np.exp(f) / np.sum(np.exp(f))
    
    '''
    f == [-1.9  0.  -6.8]
    p == [0.12998254 0.86904954 0.00096793]
    '''
    ```

  - Cross entropy loss

    <img src="/assets/images/è®¡è§†/myNote/pic01/Screen Shot 2021-07-25 at 3.34.28 PM.png" alt="Screen Shot 2021-07-25 at 3.34.28 PM" style="zoom:40%;" />

    ```python
    CrossEntropyLoss = -np.log10(p)
    '''
    CrossEntropyLoss == [0.88611498 0.06095547 3.01415795]
    '''
    ```

    - `CrossEntropyLoss` ì˜ ìµœì†Œ/ìµœëŒ€ê°’ :  1 / âˆ 

       <img src="/assets/images/è®¡è§†/myNote/pic01/cs231n screenshot.png" alt="cs231n screenshot" style="zoom:80%;" /> 

      (ì¦‰, scoreë¥¼ ì˜ ë§ì¶”ë©´ lossëŠ” 0ì— ê°€ê¹Œì›€ , ì˜ ëª» ë§ì¶”ì—ˆë‹¤ë©´ lossëŠ” âˆì— ê°€ê¹Œì›€)

    - íŠ¹ì„± : SVMì— ë¹„í•´ ë¯¸ì„¸í•œ ë°ì´í„° ë³€ê²½ì— í™•ì‹¤íˆ ì˜ˆë¯¼í•˜ê³  lossê°’ì— ë³€ë™ì´ ìˆìŒ



#### Weight Regularization

- 0ì´ë¼ëŠ” lossê°’ì„ ë§Œë“œëŠ” weightê°€ ìœ ì¼í•˜ì§€ ì•ŠìŒ

- ìœ ì¼í•œ weightê°’ì„ ê²°ì •í•´ì£¼ê¸° ìœ„í•´ì„œ Regularizationë¥¼ ë„ì…

   <img src="/assets/images/è®¡è§†/myNote/pic01/Screen Shot 2021-07-24 at 11.59.46 PM.png" alt="Screen Shot 2021-07-24 at 11.59.46 PM" style="zoom:35%;" />

  - Data Loss : í•™ìŠµìš© ë°ì´í„°ë“¤ì— ìµœëŒ€í•œ ìµœì í™”ë¥¼ í•˜ë ¤ê³  ë…¸ë ¥í•¨

    - Classifier ë‹¤í•­ì‹ ì°¨ìˆ˜ê°€ ê¹Šì–´ì§€ê²Œë¨

  - Regularization : í…ŒìŠ¤íŠ¸ ë°ì´í„°ë“¤ì— ìµœëŒ€í•œ ì¼ë°˜í™”ë¥¼ í•˜ë ¤ê³  ë…¸ë ¥í•¨

    - Classifier ë‹¤í•­ì‹ ì°¨ìˆ˜ê°€ ê¹Šì–´ì§€ì§€ ì•Šë„ë¡ ë°©ì§€í•¨
    - Î» : Regularization ì ìš© ê°€ì¤‘ì¹˜

  - Data Loss ê³¼ Regularizationê°€ ì„œë¡œ ì‹¸ìš°ë©° ê²°êµ­ ê°€ì¥ ìµœì í™”ëœ weightê°’ì„ ì¶”ì¶œí•˜ê²Œ ë¨

  - Ex) L2 Regularization

     <img src="/assets/images/è®¡è§†/myNote/pic01/Screen Shot 2021-07-25 at 12.16.41 AM.png" alt="Screen Shot 2021-07-25 at 12.16.41 AM" style="zoom:30%;" />

    - L2 Regularization ê²½ìš°, í•œìª½ìœ¼ë¡œ ì ë¦¬ê¸° ë³´ë‹¤ëŠ” ì„œë¡œ ë¹„ìŠ·í•˜ê²Œ spread outëœ Weight ë²¡í„°ë¥¼ ì„ í˜¸

      :arrow_right: Classifier ë‹¤í•­ì‹ì˜ ì „ì²´ ì°¨ìˆ˜ë¥¼ 0ì— ê°€ê¹ë„ë¡ ìœ ë„ 

    - L1 Regularization ê²½ìš°, í•œìª½ìœ¼ë¡œ ì ë ¤ ìˆê³  ë‚˜ë¨¸ì§€ëŠ” 0ì´ ë˜ëŠ” sparseí•œ Weight ë²¡í„°ë¥¼ ì„ í˜¸ 

      :arrow_right:  Classifier ë‹¤í•­ì‹ì˜ íŠ¹ì • ì›í•˜ëŠ” ì°¨ìˆ˜ë¥¼ 0ì´ ë˜ë„ë¡ ë§Œë“¬



#### Optimization

- Lossë¥¼ minimizeí•˜ëŠ” weightë¥¼ ì°¾ì•„ê°€ëŠ” ê³¼ì •

- Gradient descent : ê²½ì‚¬ë¥¼ ë”°ë¼ì„œ ì¡°ê¸ˆì”© ë‚´ë ¤ê°€ëŠ” ë°©ì‹ìœ¼ë¡œ weightë¥¼ ì—…ë°ì´íŠ¸í•¨

  ```python
  # Full-batch gradient descent : í›ˆë ¨ë°ì´í„° ì „ì²´ë¥¼ í™œìš©
  while True :
    weight_gradient = evaluate_gradient(loss_func, data, weight)
    weight = weight - learning_rate * weight_gradient # weight ì—…ë°ì´íŠ¸
  ```

  ```python
  # Mini-batch gradient descent : í›ˆë ¨ë°ì´í„°ì˜ ì¼ë¶€ë§Œì„ í™œìš© (ì„±ëŠ¥ì„ ë†’ì„)
  while True:
    data_batch = sample_training_data(data, 256) # sample 256 examples
    weight_gradient = evaluate_gradient(loss_fun, data_batch, weights)
    weight = weight - learning_rate * weight_gradient # weight ì—…ë°ì´íŠ¸
  ```

  - Mini-batch size : CPU/GPU í™˜ê²½ì— ë”°ë¼ ì„¤ì •í•¨

  - Weightë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ë˜ ë‹¤ë¥¸ ë°©ë²•ë“¤ :

    - momentum : loss ê°€ ì¤„ì–´ë“œëŠ” ì†ë„ê¹Œì§€ ê³ ë ¤, ë” ë¹ ë¥¸ íš¨ê³¼ê°€ ìˆìŒ

    - Adagrad

    - RMSProp

    - Adam

      ...



#### Backpropagation

- Computational graphë¥¼ ì´ìš©í•´ì„œ í•¨ìˆ˜ë¥¼ í‘œí˜„í•˜ê²Œ ë˜ë©´, ëª¨ë“  ë³€ìˆ˜ì— ëŒ€í•´ì„œ ì¬ê·€ì ìœ¼ë¡œ gradientë¥¼ ê³„ì‚°í•˜ëŠ” backpropagationì„ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë¨

<img src="/assets/images/è®¡è§†/myNote/pic01/cs231n 13-34 screenshot.png" alt="cs231n 13-34 screenshot" style="zoom:33%;" />

- Local Gradiant : Forward pass í•  ë•Œ êµ¬í•´ì„œ ë©”ëª¨ë¦¬ì— ì €ì¥í•´ ë‘”ë‹¤

- Global Gradiant : Backward pass í•  ë•Œ êµ¬í•¨ 

- Gradiant = Local Gradiant Ã— Global Gradiant

- ***Example***

   <img src="/assets/images/è®¡è§†/myNote/pic01/Screen Shot 2021-07-26 at 10.50.27 AM.png" alt="Screen Shot 2021-07-26 at 10.50.27 AM" style="zoom:20%;" />

  

  - Backward pass í•˜ë©´ì„œ gradient ê³„ì‚°

  <img src="/assets/images/è®¡è§†/myNote/pic01/Screen Shot 2021-07-26 at 12.21.39 PM.png" alt="Screen Shot 2021-07-26 at 12.21.39 PM" style="zoom:80%;" />

  

#### Neural Network

##### Neuron

<img src="/assets/images/è®¡è§†/myNote/pic01/Screen Shot 2021-07-26 at 1.39.00 PM.png" alt="Screen Shot 2021-07-26 at 1.39.00 PM" style="zoom:40%;" />

```python
class Neuron(object):
  # ... 
  def forward(self, inputs):
    """ assume inputs and weights are 1-D numpy arrays and bias is a number """
    cell_body_sum = np.sum(inputs * self.weights) + self.bias
    firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function
    return firing_rate
```



##### Architectures

- Fully Connected Layers : ëª¨ë“  node (or Neuron)ë“¤ì´ ì—°ê²° ë˜ì–´ìˆë‹¤

- ***Example*** : 3-Layer Neural Network (or *2-Hidden-Layer Neural Network*)

   <img src="/assets/images/è®¡è§†/myNote/pic01/Screen Shot 2021-07-26 at 2.04.33 PM.png" alt="Screen Shot 2021-07-26 at 2.04.33 PM" style="zoom:30%;" />

  ```python
  # forward-pass of a 3-layer neural network:
  f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)
  x = np.random.randn(3, 1) # random input vector of three numbers (3x1)
  h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1)
  h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4x1)
  out = np.dot(W3, h2) + b3 # output neuron (1x1)
  ```

- ë°ì´í„°ì˜ Overfittingì´ ì¼ì–´ë‚˜ì§€ ì•Šë„ë¡, Neural Networkë¥¼ êµ¬ì„±í•˜ëŠ” ë°©ë²•ì€ Networkë¥¼ ì‘ê²Œ ë§Œë“œëŠ”ê²Œ ì•„ë‹ˆë¼  Regularization Strength(Î»)ë¥¼ ë” ë†’í˜€ì¤˜ì•¼ëœë‹¤

- ê·¸ë˜ì„œ Neural NetworkëŠ” Regularization ë¥¼ ì˜ í•œë‹¤ëŠ” ì „ì œ í•˜ì— networkê°€ ë” í¬ë©´ í´ ìˆ˜ë¡ ì¢‹ë‹¤

   

##### Activation Function

- í™œì„±í™”(Activate)ë¼ëŠ” ì´ë¦„ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´ í™œì„±í™” í•¨ìˆ˜ë€ ì…ë ¥ ì‹ í˜¸ì˜ ì´í•© `cell_body_sum`ì´ í™œì„±í™”ë¥¼ ì¼ìœ¼í‚¤ëŠ”ì§€ ì •í•˜ëŠ” ì—­í• ì„ í•œë‹¤

- Activation Functionë¥¼ ì™œ ë°˜ë“œì‹œ ì´ìš©í•´ì•¼ë˜ë‚˜?

   - Activation Functionë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ Neural Network ì „ì²´ê°€ ëª‡ê°œì˜ Layerë¥¼ ê°€ì§€ë“  ê°„ì— ë‹¨ì¼ì˜ linear classifier ìˆ˜ì¤€ì´ ë˜ë²„ë¦¼

   -  ***Example*** :  y=axë¥¼ Activation Functionìœ¼ë¡œ ì‚¬ìš©í•˜ê²Œ ëœë‹¤ë©´

      - 2ê³„ì¸µ layerì¼ ê²½ìš° y=a(ax),  3ê³„ì¸µ layerì¼ ê²½ìš° y=a(a(ax)) ... nê³„ì¸µ layerì¼ ê²½ìš° y=(a^n)x 

         

- ì¢…ë¥˜ë“¤ :

   <img src="/assets/images/è®¡è§†/myNote/pic01/Screen Shot 2021-07-26 at 1.56.41 PM.png" alt="Screen Shot 2021-07-26 at 1.56.41 PM" style="zoom:20%;" />
  
  - ë‹¤ë¥¸ ë‰´ëŸ°ì—ê²Œ ìê·¹ì„ ë³´ë‚¼ì§€, ì•„ë‹ì§€ë¥¼ ê²°ì •í•˜ëŠ” ê²Œ Neural networkì—ì„œì˜ Activation functionê³¼ ê°™ìœ¼ë©°, ì‹¤ì œ ë‰´ëŸ°ê³¼ ê°€ì¥ ë¹„ìŠ·í•œ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì€ ë°”ë¡œ ReLU
  
  - **Sigmoid**
  
    - ë„“ì€ ë²”ìœ„ì˜ ìˆ«ìë¥¼ [0,1]ì‚¬ì´ë¡œ squash í•´ì¤Œ
  
    - ìš”ì¦˜ ê±°ì˜ ì˜ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ
  
    - ë¬¸ì œì  : 
  
      1. Vanishing gradient
  
          <img src="/assets/images/è®¡è§†/myNote/pic01/Screen Shot 2021-07-26 at 3.16.56 PM.png" alt="Screen Shot 2021-07-26 at 3.16.56 PM" style="zoom:25%;" />
  
         <span style="color:blue">Active region of sigmoid</span> : sigmoidê°€ ì‹¤ì œë¡œ Activeí•˜ê²Œ ì—­í™œí•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ 
  
         <span style="color:red">Saturated regime</span> : local gradientê°€ 0ì„ ê°€ì ¸ backpropagationì´ ë©ˆì¶”ê²Œë¨
  
      2. Not zero-centered (ëª¨ë“  yê°’ì´ 0 ì´ìƒì„)
  
         zig zag ë°©ì‹ìœ¼ë¡œ ì´ë™í•˜ê²Œë˜ì„œ ê²°ê³¼ì ìœ¼ë¡œ ë§¤ìš° ëŠë¦¬ê²Œ ìˆ˜ë ´ë¨ -- ìµœì ì˜ weightë¥¼ ì°¾ëŠ”ê²Œ ëŠë¦¼
  
      3. Exp( ) ì—°ì‚°ì ìœ¼ë¡œ hight cost
  
  - **Hyperbolic Tangent (tanh)**
  
    - ë„“ì€ ë²”ìœ„ì˜ ìˆ«ìë¥¼ [-1,1]ì‚¬ì´ë¡œ squash í•´ì¤Œ
    - Zero-centered (yê°’ì˜ ì¤‘ê°„ê°’ì´ 0ì„)
    - ë¬¸ì œì  :
      1. ì—¬ì „íˆ <span style="color:red">Saturated regime</span> ì´ ì¡´ì¬í•¨ 
  
  - **Rectified Linear Unit (ReLU)**
  
    - xê°€ positiveì¸ ì§€ì—­ (ê·¸ë˜í”„ ì œ1ì‚¬ë¶„ë©´)ì—ì„  <span style="color:red">Saturated regime</span> ê°€ ì¡´ì¬í•˜ì§€ :x:
    - ì—°ì‚°ì´ ë§¤ìš° íš¨ìœ¨ì 
    - Sigmoid/tanhì— ë¹„í•´ 6ë°° ì´ìƒ ë¹ ë¥´ê²Œ ìˆ˜ë ´  -- ìµœì ì˜ weightë¥¼ ì°¾ëŠ”ê²Œ ë¹„êµì  ë¹ ë¦„
    - ë¬¸ì œì  :
      1. Not zero-centered 
      2. xê°€ 0ë³´ë‹¤ ì ì€ ì§€ì—­ì€ gradientê°€ 0ì´ ë˜ì„œ Vanishing gradientê°€ ë°œìƒ 
  
  - **Leaky ReLU**
  
    - ReLUì™€ëŠ” ë‹¤ë¥´ê²Œ xê°€ 0ë³´ë‹¤ ì ì€ ì§€ì—­ì—ë„ gradientë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìŒ
  
      :arrow_right: ì¦‰, backpropagationì´ ë©ˆì¶œì¼ì´ ì—†ìŒ
  
  - **Exponential Linear Units (ELU)**
  
    - ReLU ì¥ì ë“¤ì„ ë‹¤ ê°€ì§€ê³  ìˆìœ¼ë©´ì„œ gradientê°€ ì£½ì§€ ì•ŠìŒ
    - Zero mean outputì— ê°€ê¹Œìš´ í˜•íƒœë¥¼ ê°€ì§
    - ë¬¸ì œì 
      1. Exp( ) ì—°ì‚°ì ìœ¼ë¡œ hight cost
  
  - **Maxout Neuron**
  
    - ì…ë ¥ xë¥¼ ë°›ì•„ë“œë¦¬ëŠ” íŠ¹ì •í•œ ê¸°ë³¸ í•¨ìˆ˜ë¥¼ ë¯¸ë¦¬ ì •ì˜í•˜ì§€ ì•ŠìŒ 
    - gradientê°€ 0ì´ ë˜ì§€ë„ ì•Šê³  backpropagationì´ ë©ˆì¶”ì§€ë„ ì•ŠìŒ
    - ë¬¸ì œì  :
      1. ë‰´ëŸ° ë§ˆë‹¤ ë‘ê°œì˜ parameterë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ì—°ì‚°ì´ ë‘ë°°ë¡œ ì¦ê°€ 
      
         

##### Data Preprocessing

<img src="/assets/images/è®¡è§†/myNote/pic01/Screen Shot 2021-07-26 at 10.09.12 PM.png" alt="Screen Shot 2021-07-26 at 10.09.12 PM" style="zoom:35%;" />

- Zero-centered : 

  - ê° ë°ì´í„°ì— í‰ê· ì„ ë¹¼ì¤Œ

    `original_data = original_data - np.mean(original_data, axis = 0)`

    âš ï¸ å‚è€ƒ ï¼š ì´ ì „ì²˜ë¦¬ ë°©ë²•ì€ Sigmoidì˜ Not zero-centered ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸°ì—” ì¶©ë¶„í•˜ì§€ ëª»í•¨

    - ì²˜ìŒì— input ì´ë¯¸ì§€ê°€ Zero-centeredë¡œ ì „ì²˜ë¦¬ë˜ë©´ 1ë²ˆì§¸ layerëŠ” í•´ê²°ì´ ë  ìˆ˜ ìˆê² ì§€ë§Œ ê·¸ ë‹¤ìŒ layerë¶€í„° ë‹¤ì‹œ ë¬¸ì œê°€ ë°˜ë³µë¨

- Normalized data : íŠ¹ì • ë²”ìœ„ì•ˆì— ë°ì´í„°ê°€ ë“¤ì–´ê°ˆ ìˆ˜ ìˆê²Œ ë§Œë“¤ê¸°

  - ê° ë°ì´í„°ì— í‘œì¤€í¸ì°¨ë¥¼ ë‚˜ëˆ ì¤Œ

    `original_data = original_data / np.std(original_data, axis = 0)`

  - ì´ë¯¸ì§€ì—ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ Zero-centeredëŠ” ìˆ˜í–‰í•˜ì§€ë§Œ NormalizationëŠ” ìˆ˜í–‰ ì•ˆí•¨

    ğŸ‘‰ ì´ë¯¸ì§€ì˜ ê° pixelëŠ” ì´ë¯¸ 0~255 ì‚¬ì´ì— ê°’ì„ ê°€ì§€ê³  ìˆìœ¼ë‹ˆê¹



##### Weight Initialization

**ë°©ë²•0. ëª¨ë“  initial weightë¥¼ 0ìœ¼ë¡œ ì„¤ì •**

- ëª¨ë“  Neuronë“¤ì€ ë‹¤ ê°™ì€ ì—°ì‚°ì„ ìˆ˜í–‰í•¨ -- ê²°êµ­, ëª¨ë“  Neuronë“¤ì´ ëª¨ë‘ ë˜‘ê°™ì´ ìƒê¸°ê²Œ ëœë‹¤

**ë°©ë²•1. Small random numbers**

- í‘œì¤€ì •ê·œë¶„í¬ì—ì„œ ìƒ˜í”Œë§í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•œ ë‹¤ìŒ ë” ì‘ì„ ê°’ì„ ìœ„í•´ ìŠ¤ì¼€ì¼ë§ í•´ì¤Œ (0.01ë¥¼ ê³±í•´ì¤Œ)

  `W = 0.01* np.random.randn(fan_in, fan_out)`

  - `fan_in` : number of inputs
  - `fan_out` : number of outputs

- Networkê°€ ì‘ì€ ê²½ìš°ì— ë¬¸ì œ ì—†ìŒ, í•˜ì§€ë§Œ Networkê°€ í° ê²½ìš° ë¬¸ì œê°€ ìƒê¹€

  - ëª¨ë“  Activatonë“¤ì´ 0ì´ ë˜ë²„ë¦¼
  - Backward passë¥¼ í•˜ê²Œëœë‹¤ë©´ Vanishing gradientê°€ ë°œìƒí•˜ê²Œ ë¨
  - ë§Œì•½ `W = 0.1* np.random.randn(fan_in, fan_out)`  ì¼ ê²½ìš° 
    - ê±°ì˜ ëª¨ë“  Neuronë“¤ì´ -1 ë˜ëŠ” 1ë¡œ Saturation ë¨
    - Gradientë“¤ë„ ëª¨ë‘ 0ì´ë˜ì„œ weight ì—…ë°ì´íŠ¸ê°€ ì¼ì–´ë‚˜ì§€ ì•ŠìŒ
  - ì¦‰, ìŠ¤ì¼€ì¼ë§ì´ ë„ˆë¬´ ì‘ìœ¼ë©´( Ã— 0.01) ì‚¬ë¼ì ¸ë²„ë¦¬ê³  ë„ˆë¬´ í¬ë©´ ( Ã— 0.1)  Saturation ë¨

**ë°©ë²•2. Xavier Initialization**

- inputì˜ ê°œìˆ˜ê°€ ë§ìœ¼ë©´ weightê°’ì´ ì‘ì•„ì§€ê³   inputì˜ ê°œìˆ˜ê°€ ì ìœ¼ë©´ weightê°’ì´ ì»¤ì§€ëŠ” ë°©ì‹ìœ¼ë¡œ ì§„í–‰í•˜ëŠ” ë°©ì‹

  `W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in)`

- í•˜ì§€ë§Œ Linear activationì´ ìˆë‹¤ëŠ” ê°€ì •í•˜ì— ì˜ ì‘ë™í•¨
  - ReLU (Non-linearí˜•) ë¥¼ ì“°ë©´ ì˜ ì‘ë™í•˜ì§€ ëª»í•¨ 
  - ReLUëŠ” ì¶œë ¥ì˜ ì ˆë°˜ì„ ì£½ì´ê³  ê·¸ ì ˆë°˜ì€ ë§¤ë²ˆ 0ì´ ë˜ê¸° ë•Œë¬¸ì— ì¶œë ¥ì˜ ë¶„ì‚°ì„ ë°˜í† ë§‰ ë‚´ë²„ë¦¼

**ë°©ë²•3. He at al., 2015**

- Activation functionì´ ReLUì¼ ê²½ìš° ìµœì ì¸ weight ì´ˆê¸°í™” ë°©ë²•

  `W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in/2)`

  - Neuronë“¤ ì¤‘ ì ˆë°˜ì´ ì—†ì–´ì§„ë‹¤ëŠ” ì‚¬ì‹¤ì„ ê³ ë ¤í•´ì„œ 2ë¡œ ë‚˜ëˆ ì¤Œ 

**ë°©ë²•4. Batch Normalization**

- ê° layerë“¤ì„ ê±°ì¹˜ë©´ì„œ ì…ë ¥ê°’ì˜ ë¶„í¬ê°€ ë‹¬ë¼ì§€ëŠ” í˜„ìƒì´ ë°œìƒí•˜ê¸° ë•Œë¬¸ì— í•™ìŠµ ë¶ˆì•ˆì •í™”ê°€ ìƒê¸´ë‹¤ê³  ì£¼ì¥

- ê·¸ë˜ì„œ ê° layerë“¤ì„ ê±°ì¹ ë•Œ ë§ˆë‹¤ ì´ê²ƒë“¤ì„ Normalizeë¥¼ í•´ì£¼ìëŠ” ë°©ì‹

  - ê° layerë§ˆë‹¤ Normalizeë¥¼ í•´ì£¼ë”ë¼ë„ ì—¬ì „íˆ ë¯¸ë¶„ê°€ëŠ¥í•œ í•¨ìˆ˜ì´ê³  forward/backward passí•˜ëŠ”ë° ë¬¸ì œì—†ìŒ

- Batch Normalizationì€ ì¼ë°˜ì ìœ¼ë¡œ Fully-connected ì™€ Activation function ì‚¬ì´ì— ìœ„ì¹˜ 

  <img src="/assets/images/è®¡è§†/myNote/pic01/Screen Shot 2021-07-28 at 2.40.04 PM.png" alt="Screen Shot 2021-07-28 at 2.40.04 PM" style="zoom:35%;" />

  

   <img src="/assets/images/è®¡è§†/myNote/pic01/Screen Shot 2021-07-27 at 10.43.31 PM.png" alt="Screen Shot 2021-07-27 at 10.43.31 PM" style="zoom:33%;" />

- ì¥ì  :

  - Learning rateê°€ ë‹¤ì†Œ í° ê°’ì„ ê°€ì§€ë”ë¼ë„ ê·¸ê±¸ í—ˆìš©í•´ì¤Œìœ¼ë¡œì¨ ë¹ ë¥¸ í•™ìŠµì„ ê°€ëŠ¥ì¼€ í•˜ëŠ” íš¨ê³¼ê°€ ìˆìŒ
  - weight ì´ˆê¸°í™”ì— ë„ˆë¬´ ì˜ì¡´í•˜ì§€ ì•Šì•„ë„ ë¨

- Batch Normalizationë¥¼ ì‚¬ìš©í•˜ë©´ Dropoutì„ ì‚¬ìš©í•  í•„ìš”ê°€ ì—†ë‹¤ê³  í•¨



##### Training (Transfer Learning)

- ìš°ë¦¬ ëª¨ë¸ì„ ì²˜ìŒë¶€í„° í•™ìŠµì‹œí‚¤ëŠ” ê²½ìš°ëŠ” ì˜ ì—†ë‹¤

- ì´ë¯¸ ì—°ê´€ëœ ë‹¤ë¥¸ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ì˜ Weightë“¤ì„ ê°€ì ¸ì™€ì„œ ì´ ê°€ì¤‘ì¹˜ë¥¼ <span style="color:blue">ìš°ë¦¬ì˜ ë°ì´í„°</span>ì— ì ìš©ì‹œí‚¤ëŠ” ë°©ë²• :

  - <span style="color:blue">Small size</span> dataset ì¼  ê²½ìš° : 

     <img src="/assets/images/è®¡è§†/myNote/pic01/Screen Shot 2021-07-26 at 2.40.15 PM.png" alt="Screen Shot 2021-07-26 at 2.40.15 PM" style="zoom:30%;" />

    - ì¦‰, fixí•œ ë¶€ë¶„ì€ featureë¥¼ ì¶”ì¶œí•´ì£¼ëŠ” ì—­í• ë¡œ ì‚¬ìš©í•˜ë©´ë¨ 

    

  - <span style="color:blue">Medium size</span> dataset ì¼  ê²½ìš° : 

     <img src="/assets/images/è®¡è§†/myNote/pic01/Screen Shot 2021-07-26 at 2.41.45 PM.png" alt="Screen Shot 2021-07-26 at 2.41.45 PM" style="zoom:30%;" />

    :small_red_triangle: Tip : <span style="color:green">top layerë“¤</span>ì˜ ê²½ìš°  learning rateì„¤ì •ì„ ì›ë˜ì˜ learning rateì˜ ì•½ 1/10ì„ ì‚¬ìš©í•˜ê³  <span style="color:orange">ì¤‘ê°„ layerë“¤</span>ì˜ ê²½ìš°  learning rateì„¤ì •ì„ ì›ë˜ì˜ learning rateì˜ ì•½ 1/100ì„ ì‚¬ìš©ì„ ì¶”ì²œ

- ì´ëŸ° pre-trained ëª¨ë¸ì„ í™œìš©í•˜ì—¬ í•™ìŠµí•˜ëŠ” ë°©ì‹ì´ ì²˜ìŒë¶€í„° í•™ìŠµì„ ì‹œí‚¤ëŠ” ë°©ì‹ë³´ë‹¤ ì„±ëŠ¥ì´ ì˜ë‚˜ì˜¤ëŠ” ì´ìœ  :

  - pre-trained ëª¨ë¸ì´ ImageNet ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨ëœ ê²ƒì´ë¼ í•˜ë©´..

    1. ImageNet ë°ì´í„°ì…‹ê³¼ ìœ ì‚¬í•œ ì´ë¯¸ì§€ë“¤ì„ classify í•´ì•¼ë  ê²½ìš°, <span style="color:green">top layerë“¤</span>ë§Œ í•™ìŠµì‹œì¼œë„ ì¢‹ì€ ì„±ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ
    2. ImageNet ë°ì´í„°ì…‹ê³¼ ê´€ë ¨ì—†ëŠ” ë°ì´í„°ì…‹ (ex. ì˜ë£Œ ë°ì´í„°) ì„ classify í•´ì•¼ë  ê²½ìš°, í•™ìŠµ ì‹œì¼œì•¼ë˜ëŠ” layerë¥¼ <span style="color:green">top layerë“¤</span> ë¶€í„°  <span style="color:orange">ì¤‘ê°„ layerë“¤</span> ê¹Œì§€ ì˜¬ë ¤ì•¼ë¨

    ğŸ¤” ê·¸ëŸ¼ <span style="color:orange">ì¤‘ê°„ layerë“¤</span> ì— ì „í˜€ê´€ê³„ ì—†ëŠ” ì´ë¯¸ì§€ (ex. ì˜ë£Œ ë°ì´í„°)ì— ëŒ€í•´ì„œ í•™ìŠµì„ ì‹œì¼°ë‹¤ê³  í•´ì„œ ì–´ë–»ê²Œ ì˜ë£Œ ë°ì´í„°ì— ëŒ€í•œ classify ì„±ëŠ¥ì´ ë” ì¢‹ì•„ì§€ëƒ?

    - ê°€ì¥ ì•ë‹¨ì˜ filterì—ì„œëŠ” Low levelì˜ featureë“¤(edge, color ë“±)ì„ ì¸ì‹í•˜ê³  ë’·ìª½ìœ¼ë¡œ ê°ˆ ìˆ˜ë¡ ì•ìª½ layerì˜ filterë“¤ê³¼ í•©ì„±ë˜ì–´ì„œ ì ì  abstractí•œ ê²ƒë“¤ì„ ì¸ì‹í•¨.

       <img src="/assets/images/è®¡è§†/myNote/pic01/bN2iA.png" alt="bN2iA" style="zoom:45%;" />

      ê·¸ë˜ì„œ Low levelì˜ featureë¥¼ ë¯¸ë¦¬ í•™ìŠµí•´ ë†“ëŠ”ë‹¤ëŠ” ê±°ëŠ” ê·¸ ì–´ë–¤ ì´ë¯¸ì§€ë¥¼ ë¶„ì„ í•  ë•Œë„ ë„ì›€ì´ ë¨ 

- ì •ë¦¬ë¥¼ í•´ë³´ë©´ :

   <img src="/assets/images/è®¡è§†/myNote/pic01/Screen Shot 2021-08-04 at 3.42.06 PM.png" alt="Screen Shot 2021-08-04 at 3.42.06 PM" style="zoom:40%;" />



##### Babysitting the Learning Process

- Learning ê³¼ì •ì„ ì–´ë–»ê²Œ ê´€ë¦¬í•˜ê³  ì‹œì‘í•´ ë‚˜ê°€ì•¼ë˜ëŠ”ì§€?

  1. ì „ì²˜ë¦¬
     - ì´ë¯¸ì§€ì— ëŒ€í•´ì„  Zero-centered ìˆ˜í–‰

  2. Neural Network Architecture ê²°ì • 
     - Hidden LayerëŠ” ëª‡ê°œë¥¼ ë‘˜ ê²ƒì¸ì§€? ê°ê°ì˜  Layerì—ëŠ” ëª‡ê°œì˜ Nodeë¥¼ ë‘˜ ê²ƒ ì¸ì§€?

  3. Lossê°’ ì²´í¬ (Sanity Check)

  4. ì¼ë¶€ training setì„ í™œìš©í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨ì‹œì¼œ Overfitting ì¼ì–´ë‚˜ëŠ” í™•ì¸ 

     - Overfitting : ìµœì¢…ì ìœ¼ë¡œ Loss ê°’ì´ ê±°ì˜ 0ì— ê°€ê¹ê²Œ ë§¤ìš° ì‘ê³  training setê³¼ validation setì— ëŒ€í•œ accuracyëŠ” 1ì¼ ê²½ìš°

     - ëª¨ë¸ì´ Overfittingì´ ì¼ì–´ë‚¬ë‹¤ë©´, ì¦‰

       ğŸ’ğŸ» Backpropergationì´ ë™ì‘ì„ ì˜í•˜ê³  ìˆëŠ” ê±¸ ì•Œ ìˆ˜ ìˆìŒ

       ğŸ’ğŸ» Weight ì—…ë°ì´íŠ¸ë„ ì œëŒ€ë¡œ ë™ì‘í•˜ê³  ìˆëŠ” ê±¸ ì•Œ ìˆ˜ ìˆìŒ

       ğŸ’ğŸ» Learning rateë„ í¬ê²Œ ë¬¸ì œê°€ ì—†ë‹¤ëŠ” ê±¸ ì•Œ ìˆ˜ ìˆìŒ

  5. Learning rate ì°¾ì•„ê°€ëŠ” ë‹¨ê³„

     - ì „ì²´ training setì„ í™œìš©í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚´

       1. Learning rateë¥¼ ë§¤ìš° ì‘ì€ ìˆ«ìë¡œ ì„¤ì •í•  ê²½ìš° (ex. `lr = 1e-6` )

          ğŸ‘‰ í˜„ìƒ : ë³´í†µ Lossê°’ì€ ì˜ ë³€í•˜ì§€ ì•ŠìŒ, ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  training setê³¼ validation setì— ëŒ€í•œ accuracyëŠ” ì¦ê°€í•˜ëŠ” ëª¨ìŠµì„ ë³´ì„

       2. Learning rateë¥¼ ë§¤ìš° í° ìˆ«ìë¡œ ì„¤ì •í•  ê²½ìš° (ex. `lr = 1e6` )

          ğŸ‘‰ í˜„ìƒ : Lossê°’ì´ NaNì„ ê°€ì§, ë˜ëŠ” ë§ˆì§€ë§‰ì— Infë¥¼ ê°€ì§

     - cross validationì„ í†µí•´ì„œ ì •í™•í•œ Learning rateì„ ê²°ì •í•œë‹¤  



##### Hyperparameter Optimization

- Cross-validation : 

  - training setìœ¼ë¡œ í•™ìŠµì‹œí‚¤ê³  validation setìœ¼ë¡œ í‰ê°€í•˜ëŠ” ë°©ë²•
  - ì „ëµ -- Coarseí•˜ê²Œ ì‹œì‘í•˜ê³  Fineí•˜ê²Œ íŠœë‹í•´ ë‚˜ê°€ì
    - Coarse : Epoch ìˆ˜ë¥¼ ì‘ì€ ìˆ«ìë¥¼ ì¤˜ì„œ ëŒ€ëµì ì¸ Hyperparameter ì„¤ì •ê°’ì— ëŒ€í•´ ê°ì„ ì¡ìŒ
    - Fine : Epoch ìˆ˜ë¥¼ ì¢€ ë” í¬ê²Œ ì¤˜ì„œ ì„¸ë¶€ì ì¸ Hyperparameter seachingì„ í•¨

- Learning rateëŠ” gradientì™€ ê³±í•´ì§€ê¸° ë•Œë¬¸ì— ì„ íƒë²”ìœ„ë¥¼ log scaleì„ ì‚¬ìš©í•˜ëŠ” í¸ì´ ì¢‹ë‹¤. 

- Random search :vs: Grid search

  - Grid search : Hyperparameterë¥¼ ê³ ì •ëœ ê°’ê³¼ ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§í•˜ëŠ” ê²ƒ

  - Random search :

    - ì²˜ìŒì— ëŒ€ëµì  ë²”ìœ„ë¥¼ ì„¤ì •  `lr = 10**random.uniform(-3, 6)` , `reg = 10**random.uniform(-5, 5)`

      â€‹	ğŸ‘‰ `10**` ì„ í•´ì¤Œìœ¼ë¡œì¨ log spaceí™”ë¥¼ í•´ì¤Œ

    - ê²°ê³¼ì— ê·¼ê±°í•˜ì—¬ ë²”ìœ„ë¥¼ ì¤„ì—¬ ë‚˜ê°

       <img src="/assets/images/è®¡è§†/myNote/pic01/cs231n53-36screenshot.png" alt="cs231n 5á„€á…¡á†¼ Training NN part 1 53-36 screenshot" style="zoom:35%;" />

      â€‹	ğŸ‘‰ `lr = 10**random.uniform(-3, -4)` , `reg = 10**random.uniform(-4, 0)` ë¡œ ë³€ê²½

  - Grid search ë³´ë‹¤ëŠ” **Random searchë¥¼ í•˜ëŠ” ê²ƒì´ ë” Good**

- Loss curve ëª¨ë‹ˆí„°ë§

   <img src="/assets/images/è®¡è§†/myNote/pic01/cs231n58-4screenshot.png" alt="cs231n 5á„€á…¡á†¼ Training NN part 1 58-4 screenshot" style="zoom:33%;" />

- Accuracy curve ëª¨ë‹ˆí„°ë§

   <img src="/assets/images/è®¡è§†/myNote/pic01/cs231n1-1-48screenshot.png" alt="cs231n 5á„€á…¡á†¼ Training NN part 1 1-1-48 screenshot" style="zoom:33%;" />

  - ìˆ˜ì¹˜ì˜ í•´ì„ì´ ê°€ëŠ¥í•´ì„œ ì„ í˜¸í•¨ 
  - Traning accuracyì™€ Validation accuracyì—ëŠ” Gapì´ ìƒê¹€
    - ë§Œì•½ Gapì´ ë„ˆë¬´ í¬ë‹¤ë©´ Overfitting ì˜ì‹¬í•´ë´ì•¼ë¨ ğŸ‘‰ regularizationë¥¼ ê°•í™”í•˜ê¸°
    - ë§Œì•½ Gapì´ ë„ˆë¬´ ì‘ë‹¤ë©´ ì•„ì§ Overfití•˜ì§€ ì•Šì€ ê²ƒì´ê³  capacityì„ ë†’í ìˆ˜ ìˆëŠ” ì¶©ë¶„í•œ ì—¬ìœ ê°€ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•¨



##### Parameter update

- Stochastic Gradient Descent(SGD)ëŠ” weightë¥¼ ì°¾ì•„ê°€ëŠ” ê³¼ì • ë§¤ìš° ëŠë¦¼

  ```python
  """
  lr : Learning rate
  dx : Gradient
  """
  while True:
   dataBatch = dataset.sample_dataBatch()
   loss = network.forward(dataBatch)
   dx = network.backward()
   x = x + (-lr*dx) # Gradient descent update
  ```

   <img src="/assets/images/è®¡è§†/myNote/gif/optimization.gif" alt="optimization" style="zoom:80%;" />

  

- <span style="font-size:1.1rem; background:yellow;">1st Order Optimization Method</span> : Loss Functionë¥¼ êµ¬í•˜ëŠ”ë° ìˆì–´ì„œ gradient ì •ë³´ë§Œ ì‚¬ìš©

  1. **Momentum**

     ```python
     # Momentum update
     velocity = mu*velocity - lr*dx
     x = x + velocity
     ```

     - `mu` : ë§ˆì°° ê³„ìˆ˜(ì ì°¨ ì†ë„ê°€ ëŠë ¤ì§€ê²Œ ì§„í–‰ë˜ë„ë¡ í•´ì¤Œ) ë³´í†µ 0.5, 0.6, ë˜ëŠ” 0.99ë¡œ ì„¤ì • 
     - ì†ë„`velocity`ë¥¼ ì¼ë‹¨ ì—…ë°ì´íŠ¸ í•´ì£¼ê³ , `x`ì˜ ìœ„ì¹˜ë¥¼ ì†ë„ë¥¼ í†µí•´ì„œ ì—…ë°ì´íŠ¸ í•´ì¤Œ
       - ì–¸ë•ì—ì„œ ê³µì„ êµ´ë¦¬ëŠ” ìƒí™©ì„ ì—°ìƒí•˜ë©´ë¨ <img src="/assets/images/è®¡è§†/myNote/pic01/cs231n6-32screenshot.png" alt="cs231n 6á„€á…¡á†¼ Training NN part 2 6-32 screenshot" style="zoom:33%;" />
       - ê²½ì‚¬ê°€ ë‚®ì€ ì§€ì ì—ì„œëŠ” ì†ë„ê°€ ì²œì²œíˆ build up ë¨
     - SGDë³´ë‹¤ ë¹ ë¥´ê²Œ ì°¾ì•„ê°

  2. **Nesterov Accelerated Gradient (NAG)**

     - Momentumë³´ë‹¤ í•­ìƒ convergence rateê°€ ë” ì¢‹ë‹¤

     - <span style="color:green">Momentum step : `mu*velocity`</span>

     - <span style="color:red">Gradient step : `lr*dx`</span>

     - <span style="color:red">Gradient step</span>ì„ ê³„ì‚°í•˜ê¸° ì „ì— <span style="color:green">Momentum step</span>ì„ ë¯¸ë¦¬ ê³ ë ¤ë¥¼ í•´ì„œ ì‹œì‘ì ì„ <span style="color:green">Momentum step</span>ì˜ ì¢…ë£Œì ìœ¼ë¡œ ë³€ê²½í•œ ë‹¤ìŒ gradient stepì„ í‰ê°€í•˜ëŠ” ë°©ì‹ 

       <img src="/assets/images/è®¡è§†/myNote/pic01/cs231n9-26screenshot.png" alt="cs231n 6á„€á…¡á†¼ Training NN part 2 9-26 screenshot" style="zoom:33%;" />

     - forward/backward ê³¼ì •ì—ì„œ ë¶ˆí¸í•œ ì  : 

        Momentum : parameter vector Î¸ ì™€ ê·¸ ìœ„ì¹˜ì—ì„œì˜ gradientë¥¼ êµ¬í•˜ê²Œë¨ 

        NAG : parameter vector Î¸ ì™€ parameter vectorì™€ ë‹¤ë¥¸ ìœ„ì¹˜ì—ì„œì˜ gradientë¥¼ ìš”êµ¬í•¨

       ğŸ’ğŸ» í•´ê²°ë°©ì•ˆ :

        <img src="/assets/images/è®¡è§†/myNote/pic01/Screen Shot 2021-07-28 at 4.15.22 PM.png" alt="Screen Shot 2021-07-28 at 4.15.22 PM" style="zoom:35%;" />

       ```python
       # Nestrov Momentum update
       velocity_prev = velocity
       velocity = mu*velocity - lr*dx
       x = x + -mu*velocity_prev + (1+mu)*velocity
       ```

  3. **AdaGrad**

     ```python
     # AdaGrad update
     cache = cache + dx**2
     x = x + (-lr*dx / (np.sqrt(cache)+1e-7))
     ```

     - cache : ê³„ì† ì¦ê°€í•˜ê²Œë˜ëŠ” ì–‘ìˆ˜, parameter vectorì™€ ë™ì¼í•œ sizeì˜ ê±°ëŒ€í•œ vector
     - Per-parameter adaptive learning rate method : ëª¨ë“  parameterë“¤ì´ ë™ì¼í•œ learning rateë¥¼ ì ìš©ë°›ëŠ”ê²Œ ì•„ë‹ˆë¼, cacheê°€ ê³„ì† building up ë˜ë©´ì„œ í•˜ë‚˜ í•˜ë‚˜ì˜ parameterë“¤ì´ ë‹¤ ë‹¤ë¥¸ ê°’ì˜ learning rateì™€ ê°™ì€ ì˜í–¥ì„ ë°›ê²Œë¨
     - `1e-7` : 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì¼ì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ì—­í• 
     - ë¬¸ì œì  : cacheê°€ ê³„ì† building up ë˜ë©´ì„œ ê²°êµ­ step sizeê°€ 0ì— ê°€ê¹Œì›Œì ¸ì„œ ì¡°ê¸°ì— í•™ìŠµì´ ì¢…ë£Œë˜ëŠ” ìƒí™©ì´ ë°œìƒí•˜ê²Œ ë¨

  4. **RMSProp**

     - AdaGrad ì—…ë°ì´íŠ¸ ì¤‘ í•™ìŠµì´ ì¢…ë£Œë˜ì§€ ì•Šê²Œ í•˜ê¸° ìœ„í•´ ì–´ë–¤ "ì—ë„ˆì§€"ë¥¼ ì œê³µ

     ```python
     # RMSProp
     cache = decay_rate*cache + (1-decay_rate)*dx**2
     x = x + (-lr*dx / (np.sqrt(cache)+1e-7))
     ```

     - `decay_rate` : stepì˜ ì†ë„ë¥¼ ê°€ì†/ê°ì† ì‹œí‚¬ ìˆ˜ ìˆë‹¤

  5. **Adam** -- *Good default choice ğŸ‘* 

     ```python
     # Adam
     m,v = #... initialize caches to zeros
     for t in xrange(1, big_number):
       m = beta1*m + (1-beta1)*dx            # Momentum-like
       v = beta2*v + (1-beta2)*(dx**2)       # RMSProp-like
       mb = m/(1-beta1**t)	# bias correction
       vb = v/(1-beta2**t) # bias correction
       x = x + (-lr*mb / (np.sqrt(vb)+1e-7)) # RMSProp-like
     ```

     - Momentum + RMSProp ê²°í•©í•œ í˜•íƒœ

     - `beta1`, `beta2` : hyperparameterë¡œì¨ ë³´í†µ 0.9, 0.99 ìœ¼ë¡œ ì„¤ì •

     -  bias correction :  ì´ˆê¸° iteration ë‹¨ê³„ì—ì„œ `m`, `v`ë¥¼ scaling up í•´ì£¼ëŠ” ì—­í• 

       

- ìœ„ì˜ ë°©ì•ˆë“¤ì´ ëª¨ë‘ learing rate `lr` ë¥¼ hyperparameterë¡œ ê°€ì§€ëŠ”ë° ì–´ë– í•œ learing rateë¥¼ ê°€ì§€ëŠ”ê²Œ ìµœì„ ì¼ê¹Œ?

  - ì´ˆê¸°ì—ëŠ” í° learing rateë¥¼ ì ìš©í•˜ê³  ì„œì„œíˆ learing rateë¥¼ ì‘ê²Œ ë§Œë“¤ì–´ ì ìš©í•  ìˆ˜ ìˆê²Œí•˜ë„ë¡

    - æ–¹æ³•1 -- Step Decay : í•œ Epochë§ˆë‹¤ learing rateë¥¼ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ê°ì†Œì‹œì¼œì£¼ëŠ” ë°©ë²•
    - æ–¹æ³•2 -- Exponential Decay : <img src="/assets/images/è®¡è§†/myNote/pic01/cs231n20-4screenshot.png" alt="cs231n 6á„€á…¡á†¼ Training NN part 2 20-4 screenshot" style="zoom:43%;" />
    - æ–¹æ³•3 -- 1/t Decay : <img src="/assets/images/è®¡è§†/myNote/pic01/cs231n20-5screenshot.png" alt="cs231n 6á„€á…¡á†¼ Training NN part 2 20-5 screenshot" style="zoom:43%;" />

    

- <span style="font-size:1.1rem; background:yellow;">2nd Order Optimization Method</span> : Hessianì„ í†µí•´ì„œ ê²½ì‚¬ë¿ë§Œ ì•„ë‹ˆë¼ ê³¡ë©´ì´ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì—ˆëŠ”ì§€ ì•Œ ìˆ˜ ìˆìŒ

  - ê³¡ë©´ì´ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì—ˆëŠ”ì§€ ì•Œë©´ í•™ìŠµ í•„ìš”ì—†ì´ ë°”ë¡œ ìµœì €ì ìœ¼ë¡œ ê°ˆ ìˆ˜ ìˆìŒ

  <img src="/assets/images/è®¡è§†/myNote/pic01/cs231n22-17screenshot.png" alt="cs231n 6á„€á…¡á†¼ Training NN part 2 22-17 screenshot" style="zoom:33%;" />

  - ì¥ì  :

    - learing rateê°€ í•„ìš” ì—†ê²Œë¨
    - ìˆ˜ë ´ì´ ë¹¨ë¼ì§„ë‹¤ 

  - Deep í•œ Neural Networkì—ì„œëŠ” ì—°ì‚°ëŸ‰ì´ ë„ˆë¬´ ë§ì•„ í˜„ì‹¤ì ìœ¼ë¡œ ì‚¬ìš©ë¶ˆê°€ :

    - ë§Œì•½ì— parameterë¥¼ 1ì–µê°œë¥¼ ê°€ì§€ëŠ” Neural Networkì¼ ê²½ìš°, HessianëŠ” 1ì–µÃ—1ì–µì˜ matrixë¥¼ ê°€ì§€ê²Œ ë¨
    - ê²Œë‹¤ê°€ ì´ matrixë¥¼ inverse í•´ì•¼ë¨ 

  - 2nd Order Optimization Methodë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì ‘ê·¼ë²•ë“¤ 

    - *BGFS* 

      ì¥ì  : Hessian matrixë¥¼ inverseí•˜ëŠ” ëŒ€ì‹ ì— rank 1ì˜ Hessianì„ inverseí•¨ìœ¼ë¡œì¨ ì—°ì‚°ì„ ì¤„ì„

      ë‹¨ì  : ì—¬ì „íˆ ë©”ëª¨ë¦¬ì— ì €ì¥, í° Networkì—ì„  ì•„ì§ ë™ì‘í•˜ê¸° ì–´ë ¤ì›€

    - *Limited memory BGFS*

      ì¥ì  : ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ì§€ :x: 

      ë‹¨ì  : ì¼ë°˜ì ìœ¼ë¡œ full-batchì¼ ë• ì˜ ë™ì‘, Mini-batch í™˜ê²½ì—ì„  ì˜ ë™ì‘ :x: 

  

  ##### Regularization

  - Dropout

     <img src="/assets/images/è®¡è§†/myNote/pic01/cs231n27-22screenshot.png" alt="cs231n 6á„€á…¡á†¼ Training NN part 2 27-22 screenshot" style="zoom:33%;" />

    ```python
    probability = 0.5  # dropoutí™•ë¥  ì„¤ì •
    def train_step(X): # XëŠ” data
      Hidden1 = np.maximum(0, np.dot(Weight1, X)+bias1)       # Activation Func : ReLU
      Mask1 = np.random.rand(*Hidden1.shape) < probability 
      Hidden1 = Hidden1 * Mask1 # 0ìœ¼ë¡œ ì„¤ì •ëœ nodeëŠ” dropë¨!
      Hidden2 = np.maximum(0, np.dot(Weight2, Hidden1)+bias2) # Activation Func : ReLU
      Mask2 = np.random.rand(*Hidden2.shape) < probability 
      Hidden2 = Hidden2 * Mask2 # 0ìœ¼ë¡œ ì„¤ì •ëœ nodeëŠ” dropë¨!
      out = np.dot(Weight3, Hidden2)+bias3
      
      # backward pass : compute gradient...
      # perform parameter update...
    ```

    - Forward pass ê³¼ì •ì—ì„œ ì¼ë¶€ ë‰´ëŸ°ì„ 0ìœ¼ë¡œ ë§Œë“œëŠ” ê²ƒ (Forward pass iteration ë§ˆë‹¤ ëª¨ì–‘ì€ ê³„ì† ë°”ë€œ)

    - `Mask` : ê° nodeê°€ 0 ì•„ë‹ˆë©´ 1ì´ ë˜ë„ë¡ ì„¤ì •í•´ì£¼ëŠ” ì—­í•  (probabilityë³´ë‹¤ ì‘ìœ¼ë©´ 0 ì•„ë‹˜ 1ì´ ë¨)

      - `numpy.random.rand() ` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬`(x1, ... ,xn)`ëª¨ì–‘ì˜  nì°¨ì› ëœë¤ ë°°ì—´ì„ ìƒì„±
      - `*Hidden.shape` : tuple unpacking <a href="https://www.w3schools.com/python/trypython.asp?filename=demo_tuple_unpacking_asterix2"> *Code Example* </a>

    - ëœë¤í•˜ê²Œ ì¼ë¶€ nodeë“¤ì„ 0ìœ¼ë¡œ ì„¤ì • = ì—°ê²°ì´ ëŠí‚¨ê±°ë‚˜ ë‹¤ë¦„ì—†ìŒ 

    - ê·¼ë° Dropì„ í•˜ëŠ”ê²Œ ì™œ ì¢‹ìŒ?

      ğŸ‘‰ Network ë‚´ì˜ ê° nodeê°€ í•œ ë¶€ë¶„ë§Œ ì§‘ì¤‘ì ìœ¼ë¡œ ê´€ì°°í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ ì•½ê°„ì”© ì¤‘ë³µì„ ê°€ì§€ë©´ì„œ ëª¨ë“  ê²ƒì„ ê°™ì´ ê´€ì°°í•˜ëŠ” ëª¨ìŠµì„ ë³´ì´ë©´ì„œ ì„±ëŠ¥ì´ ì¢‹ì•„ì§

       <img src="/assets/images/è®¡è§†/myNote/pic01/cs231n30-58screenshot.png" alt="cs231n 6á„€á…¡á†¼ Training NN part 2 30-58 screenshot" style="zoom:30%;" />

      ğŸ‘‰ Overfittingì„ ì–´ëŠì •ë„ ë§‰ì•„ì¤€ë‹¤. 

      ğŸ‘‰ ë‹¨ì¼ ëª¨ë¸ë¡œ ì•™ìƒë¸”(*ì—¬ëŸ¬ ê°œì˜ ëª¨ë¸* ì—ì„œ ë‚˜ì˜¨ ê°’ì„ í‰ê· í•˜ëŠ”ê±°) íš¨ê³¼ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤

      â€‹	  *ì—¬ëŸ¬ ê°œì˜ ëª¨ë¸* = Forward pass iteration ë§ˆë‹¤ ëª¨ì–‘ì´ ê³„ì† ë°”ë€œ

    - Testing stage :

      ğŸ’ğŸ» *Monte Carlo approximation ë°©ë²•* : Testing stageì—ë„ Dropoutë¥¼ ê·¸ëŒ€ë¡œ í™œìš©, ê·¸ë˜ì„œ ê°ê°ì˜ Dropoutì— ëŒ€í•´ì„œ í‰ê· ì„ ë‚¸ ì˜ˆì¸¡ë°©ë²• , ë¹„íš¨ìœ¨ì ì¸ ë°©ë²•

      ğŸ’ğŸ» *Dropoutì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë°©ë²•* : 

      â€‹	Testing stage ë•ŒëŠ” ëª¨ë“  neuronë“¤ì€ turn on!

      â€‹	ëŒ€ì‹  activation ê°’ë“¤ì„ training stage ë•Œì˜ `probability` ë§Œí¼ scalingì„ í•´ì¤˜ì•¼ë¨

      ```python
      def predict(X):
        Hidden1 = np.maximum(0, np.dot(Weight1, X)+bias1) * probability
        Hidden2 = np.maximum(0, np.dot(Weight2, Hidden1)+bias2) * probability
        out = np.dot(Weight3, Hidden2)+bias3
      ```

      ì´ìœ  : Training stage / Testing stage ë•Œ ì˜ ì˜ˆì¸¡ê°’ì´ ì„œë¡œ ìƒì´í•˜ë‹¤.

      â€‹	***Example.*** `probability` ê°€ 0.5 (== 1/2)ì¼ ê²½ìš°

      â€‹	<img src="/assets/images/è®¡è§†/myNote/pic01/Screen Shot 2021-07-29 at 2.16.58 PM.png" alt="Screen Shot 2021-07-29 at 2.16.58 PM" style="zoom:30%;" />

      â€‹	ğŸ’ğŸ» *Inverted Dropout ë°©ë²•* : 

      - Scalingì„ Testing stage ì— ì•ˆí•˜ê³  Training stage ì— ì²˜ë¦¬í•´ì¤Œ

      ```python
      probability = 0.5  
      def train_step(X): 
        Hidden1 = np.maximum(0, np.dot(Weight1, X)+bias1) / probability       # Scaling      
        Mask1 = np.random.rand(*Hidden1.shape) < probability 
        Hidden1 = Hidden1 * Mask1 
        Hidden2 = np.maximum(0, np.dot(Weight2, Hidden1)+bias2) / probability # Scaling 
        Mask2 = np.random.rand(*Hidden2.shape) < probability 
        Hidden2 = Hidden2 * Mask2 
        out = np.dot(Weight3, Hidden2)+bias3
        
        # backward pass : compute gradient...
        # perform parameter update...
        
       def predict(X):
        Hidden1 = np.maximum(0, np.dot(Weight1, X)+bias1)       # No Scaling 
        Hidden2 = np.maximum(0, np.dot(Weight2, Hidden1)+bias2) # No Scaling 
        out = np.dot(Weight3, Hidden2)+bias3
      ```

    - ì°¸ê³  : ìµœê·¼ì— Batch Normalizationì“°ëŠ” ê²½ìš° Dropoutë¥¼ ì˜ ì‚¬ìš©í•˜ì§€ :x: 

  

***

ã€ŠReferenceã€‹

1.  <a href="https://www.youtube.com/watch?v=5t1E3LZ3FDY&list=PL1Kb3QTCLIVtyOuMgyVgT-OeW0PYXl3j5&index=6">Youtube</a>
2. <a href="https://leechamin.tistory.com/98?category=830805">Blog</a>
3. <a href="https://cs231n.github.io/">Course Site</a>

***

